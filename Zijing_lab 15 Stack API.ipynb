{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackAPI\n",
    "\n",
    "#### Import the necessary libraries here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stackapi in /Users/carrie/Library/Python/3.8/lib/python/site-packages (0.2.0)\n",
      "Requirement already satisfied: six in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from stackapi) (1.15.0)\n",
      "Requirement already satisfied: requests in /Users/carrie/Library/Python/3.8/lib/python/site-packages (from stackapi) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/carrie/Library/Python/3.8/lib/python/site-packages (from requests->stackapi) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/carrie/Library/Python/3.8/lib/python/site-packages (from requests->stackapi) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/carrie/Library/Python/3.8/lib/python/site-packages (from requests->stackapi) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/carrie/Library/Python/3.8/lib/python/site-packages (from requests->stackapi) (2021.10.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stackapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#stack api is the api wranggler for stackoverflow \n",
    "#documentation: https://stackapi.readthedocs.io/en/latest/user/intro.html\n",
    "\n",
    "from stackapi import StackAPI\n",
    "\n",
    "#retrieve the 500 most recent comments from Stack Overflow \n",
    "SITE = StackAPI('stackoverflow')\n",
    "#comments = SITE.fetch('comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Find the questions and answers of last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get timestamp of a date\n",
    "import time\n",
    "import datetime\n",
    "s1 = \"01/11/2011\"\n",
    "s2 = \"30/11/2021\"\n",
    "d1=int(time.mktime(datetime.datetime.strptime(s1, \"%d/%m/%Y\").timetuple()))\n",
    "d2=int(time.mktime(datetime.datetime.strptime(s2, \"%d/%m/%Y\").timetuple()))\n",
    "#print(d1)\n",
    "#print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Why is processing a sorted array faster than processing an unsorted array?\n",
      "Question : Why does HTML think “chucknorris” is a color?\n",
      "Question : How do I find all files containing specific text on Linux?\n",
      "Question : How to return the response from an asynchronous call\n",
      "Question : For-each over an array in JavaScript\n",
      "Question : &quot;Thinking in AngularJS&quot; if I have a jQuery background?\n",
      "Question : How do I exit the Vim editor?\n",
      "Question : What&#39;s the difference between tilde(~) and caret(^) in package.json?\n",
      "Question : How is Docker different from a virtual machine?\n",
      "Question : How can I pair socks from a pile efficiently?\n"
     ]
    }
   ],
   "source": [
    "#questions = SITE.fetch('questions', fromdate=d1, todate=d2, min=500, body=True, sort='votes')\n",
    "#print(questions)\n",
    "\n",
    "SITE.max_pages=1\n",
    "SITE.page_size=10\n",
    "\n",
    "questions = SITE.fetch('questions', fromdate=d1, todate=d2, min=100, sort='votes')\n",
    "#answers = SITE.fetch('answers', min=20, sort='votes')\n",
    "\n",
    "#questions = SITE.fetch('questions', filter='withbody', min=100, sort='votes')\n",
    "\n",
    "#print(questions)\n",
    "\n",
    "for q in questions['items']:\n",
    "    title = q['title']\n",
    "    print(f'Question : {title}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#couldn't find the right way to find the correponding to the answer above....\n",
    "\n",
    "SITE.max_pages=1\n",
    "SITE.page_size=10\n",
    "\n",
    "#    top_answer = SITE.fetch(answers , order = 'desc', sort='votes')\n",
    "#    print('Most Voted Answer :'.format(top_answer['items'][0]['answers']))\n",
    "\n",
    "# question = SITE.fetch('questions/{ids}', ids=question_id, filter='withbody')\n",
    "# answers = SITE.fetch('questions/{ids}/answers', ids=question_id, filter='withbody')\n",
    "# #print(answers)\n",
    "# for a in answers['items']:\n",
    "#     body=a['body']\n",
    "#     print('Answers : {0}'.format(get_text(body)))\n",
    "# #print(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Find the most voted question today with at least a score of 5 and tagged with 'python'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Multiprocessing - Pipe vs Queue\n",
      "Question : Python, want logging with log rotation and compression\n",
      "Question : AppEngine bulkloader, high replication store and python27 runtime\n",
      "Question : python map string split list\n",
      "Question : How to convert a python set to a numpy array?\n",
      "Question : How to make a field conditionally optional in WTForms?\n",
      "Question : Django circular model reference\n",
      "Question : Using request.user with Django ModelForm\n",
      "Question : How do I merge results from target page to current page in scrapy?\n",
      "Question : Making first name, last name a required attribute rather than an optional one in Django&#39;s auth User model\n",
      "Question : rotate text around its center in pycairo\n",
      "Question : Python case insensitive file name?\n",
      "Question : scikits.audiolab on Ubuntu Oneiric - ImportError: No module named _sndfile\n",
      "Question : What makes (open) Dylan distinct from other programming languages?\n",
      "Question : How to change ticks on a histogram? (matplotlib)\n",
      "Question : What should I do if socket.setdefaulttimeout() is not working?\n",
      "Question : Assignment of objects and fundamental types\n",
      "Question : What is the proper way to handle Redis connection in Tornado ? (Async - Pub/Sub)\n",
      "Question : How can I struct.unpack many numbers at once\n",
      "Question : making hexbin in matplotlib python fill in empty space on a square axis?\n",
      "Question : Masks in python opencv cv2 not working?\n",
      "Question : How to render my select field with WTForms?\n",
      "Question : Django - Creating &amp; Store PDF Files using XHTML2PDF\n",
      "Question : use multiprocessing as local IPC\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "s3=\"11/12/2011\"\n",
    "s4=\"12/12/2011\"\n",
    "\n",
    "d3=int(time.mktime(datetime.datetime.strptime(s3, \"%d/%m/%Y\").timetuple()))\n",
    "d4=int(time.mktime(datetime.datetime.strptime(s4, \"%d/%m/%Y\").timetuple()))\n",
    "\n",
    "SITE.max_pages=1\n",
    "SITE.page_size=100\n",
    "\n",
    "questions = SITE.fetch('questions', fromdate=d3, todate=d4, min=5, tagged='python', sort='votes')\n",
    "questions\n",
    "\n",
    "for i in questions['items']:\n",
    "    title = i['title']\n",
    "    print(f'Question : {title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Find the answers with id 6784 and 6473."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6784: The sound pressure decreases by 1/r. So a doubling of the distance results in a 6 dB lower amplitude. This should be easy to model by a distance dependent amplification.\n",
      "\n",
      "The interesting part of the problem is the sound absorption caused by air. This absorption is frequency dependent (it is higher for high frequencies) and also depends or air pressure, humidity and temperature. You can find a detailed quantitative model in the ISO 9613-1 standard.    \n",
      "\n",
      "6784: Your findMaxAmplitude only looks at the positive excursions. It should use something like\n",
      "\n",
      "<pre><code>max = (short)Math.Max(max, Math.Abs(value));\n",
      "</code></pre>\n",
      "\n",
      "Your normalization seems quite involved. A simpler version would use:\n",
      "\n",
      "<pre><code>return (short)Math.Round(value * targetMax / rawMax);\n",
      "</code></pre>\n",
      "\n",
      "Whether a targetMax of 8000 is correct is a matter of taste. Normally I would expect normalisation of 16-bit samples to use the maximum range of values. So a targetMax of 32767 seems more logical.\n",
      "The normalization should probably be done after the LPF operation, as the gain of the LPF may change the maximum value of your sequence.\n",
      "\n",
      "6784: The ASIO API defines the following call on the ASIO driver:\n",
      "\n",
      "<pre><code>ASIOError ASIOControlPanel(void);\n",
      "</code></pre>\n",
      "\n",
      "Use this to show the control panel.\n",
      "\n",
      "6784: When measuring the level of a sound signal, you should calculate the dB from the RMS value. In your sample you are looking at the absolute peak level. A single (peak) sample value determines your dB value, even when all other samples are exactly 0.\n",
      "\n",
      "try this:\n",
      "\n",
      "<pre><code>double sum = 0;\n",
      "for (var i = 0; i &lt; _buffer.length; i = i + 2)\n",
      "{\n",
      "    double sample = BitConverter.ToInt16(_buffer, i) / 32768.0;\n",
      "    sum += (sample * sample);\n",
      "}\n",
      "double rms = Math.Sqrt(sum / (_buffer.length / 2));\n",
      "var decibel = 20 * Math.Log10(rms);\n",
      "</code></pre>\n",
      "\n",
      "For 'instantaneous' dB levels you would normally calculate the RMS over a segment of 20-50 ms.\n",
      "Note that the calculated dB value is relative to full-scale. For sound the dB value should be related to 20 uPa, and you will need to calibrate your signal to find the proper conversion from digital values to pressure values.\n",
      "\n",
      "6784: The waveOut API only accepts interleaved samples for the different channels, so you will have to merge the 2 single channel files into a dual channel file and play that.\n",
      "\n",
      "6784: The problem and solution are explained in more detail here: <a href=\"http://social.msdn.microsoft.com/Forums/en-US/winforms/thread/77fbec3b-1f63-42e1-a200-19b261b63794/\" rel=\"nofollow\">Dev center forums</a>.\n",
      "This also explains why setting Me.Focus helps fixing the problem.\n",
      "\n",
      "6784: If you use the maximum amplitude (2), then your volume level would be determined by a single sample (which you might not even be able to hear). When calculating a value that correlates with your impression of the loudness of the sound such as the Sound Pressure Level or the Sound Power Level you need to use the RMS (1).\n",
      "\n",
      "Because you ear is not equally sensitive to all frequencies, a better correlate of your perception can be had by using an <a href=\"http://en.wikipedia.org/wiki/A-weighting\" rel=\"nofollow\">A-weighting</a> on the signal. Split (filter) the signal in octave bands, calculate the RMS for each band and apply the A-weighting. \n",
      "\n",
      "6784: I don't know if this is still print: \"Voice and speech processing' by Thomas Parsons. ISBN 0-07-048541-0. If you search for this book in Google books you'll quickly find related/equivalent works. \n",
      "\n",
      "6784: The CF_WAVE clipboard format is simply a .wav file. If you load a .wav file into memory, then you can copy it to the clipboard using code like this:\n",
      "\n",
      "<pre><code>                DWORD BytesRead;\n",
      "            void* pFData = GlobalLock(hFData);\n",
      "            if (ReadFile(hFile,pFData,GetFileSize(hFile,NULL),&amp;BytesRead,NULL) == FALSE)\n",
      "            {\n",
      "                ErrCode = E_FILE_READ;\n",
      "            }\n",
      "            GlobalUnlock(hFData);\n",
      "</code></pre>\n",
      "\n",
      "...\n",
      "\n",
      "<pre><code>        if (OpenClipboard(NULL))\n",
      "    {\n",
      "        EmptyClipboard();\n",
      "        if (SetClipboardData(CF_WAVE,hFData) == NULL)\n",
      "        {\n",
      "            GlobalFree(hFData);\n",
      "        }\n",
      "        CloseClipboard();\n",
      "    }\n",
      "    else\n",
      "    {\n",
      "</code></pre>\n",
      "\n",
      "Here, hFile the the WAV file handle.\n",
      "\n",
      "6784: You could use (full- or third-) octave band filters and use tabulated values to perform the A- or C- weighting for each band. Calculate the signal level in each band and add the tabulated weighting values found <a href=\"http://www.sengpielaudio.com/calculator-dba-spl.htm\" rel=\"nofollow\">here</a>.\n",
      "\n",
      "6784: mmioDescend begins searching at the current file position. So it will not find the chunk you have just written. You should first reset the file position to the start, or to a previous chunk.\n",
      "Did you set the other chunk fields (ckSize and fccType) correctly? \n",
      "Perhaps you should perform an mmioAscend after creating the chunk to get the padding right.\n",
      "\n",
      "6784: One way to do this would be to resample the signal from 44100 Hz to 48000 Hz, so both signals have the same samplerate, and perform a cross-correlation. The shape of the cross-correlation could be a measure of similarity. You could look at the height of the peak, or the ratio of energy in the peak to the total energy.\n",
      "\n",
      "Note however that when the signal repeats itself, you will get multiple cross-correlation peaks.\n",
      "\n",
      "6784: In sound, decibel values are referenced to a sound pressure level of 20µPa (20 micro Pascal).\n",
      "So in your case the reference_amplitude would be the amplitude generated by your microphone in the presence of a sound field with a level of 20µPa. \n",
      "\n",
      "In practice, to find this level, microphones are often calibrated (using a microphone calibrator) with a signal of some precisely known level (often around 94dB). The amplitude resulting from this calibration signal can then be used to calculate the amplitude for the reference signal (assuming the response of the microphone is linear).   \n",
      "\n",
      "6784: To change the pitch, you'll have to perform an <a href=\"http://en.wikipedia.org/wiki/FFT\" rel=\"nofollow\">FFT</a> on a number of frames and then shift the data in frequency (move the data to different frequency bins) and perform an inverse FFT.\n",
      "\n",
      "If you don't mind the sound fragment getting longer while lowering the pitch (or higher when increasing the pitch), you could <a href=\"http://en.wikipedia.org/wiki/Sample_rate_conversion\" rel=\"nofollow\">resample</a> the frames. For instance, you could double each frame (insert a copy of each frame in the stream) thereby lowering the playback speed and the pitch. You can then improve the audio quality by improving the resampling algorithm to use some sort of interpolation and/or filtering. \n",
      "\n",
      "6784: I do not know whether a formal definition of a frame exists, but when referring to an audio  frame we usually mean a single time sample of a number of channels. So 2 audio channels @ 8 bits per channel results in 2 bytes per frame. 4 channels @ 16 bit per sample is 8 bytes. \n",
      "\n",
      "6784: There's a lot of filter source code to be found <a href=\"http://www.musicdsp.org/archive.php?classid=3\" rel=\"nofollow\">here</a>\n",
      "\n",
      "6784: Here we go for the bounty :)\n",
      "\n",
      "To find a particular reference signal in a larger audio fragment, you need to use a cross-correlation algorithm. The basic formulae can be found in this <a href=\"http://en.wikipedia.org/wiki/Cross_correlation\" rel=\"nofollow\">Wikipedia article</a>.\n",
      "\n",
      "Cross-correlation is a process by which 2 signals are compared. This is done by multiplying both signals and summing the results for all samples. Then one of the signals is shifted (usually by 1 sample), and the calculation is repeated. If you try to visualize this for very simple signals such as a single impulse (e.g. 1 sample has a certain value while the remaining samples are zero), or a pure sine wave, you will see that the result of the cross-correlation is indeed a measure for for how much both signals are alike and the delay between them. Another article that may provide more insight can be found <a href=\"http://paulbourke.net/miscellaneous/correlate/\" rel=\"nofollow\">here</a>. \n",
      "\n",
      "This <a href=\"http://paulbourke.net/miscellaneous/correlate/\" rel=\"nofollow\">article by Paul Bourke</a> also contains source code for a straightforward time-domain implementation. Note that the article is written for a general signal. Audio has the special property that the long-time average is usualy 0. This means that the averages used in Paul Bourkes formula (mx and my) can be left out.\n",
      "There are also fast implementations of the cross-correlation based on the FFT (see <a href=\"http://www.alglib.net/\" rel=\"nofollow\">ALGLIB</a>).\n",
      "\n",
      "The (maximum) value of the correlation depends on the sample values in the audio signals. In Paul Bourke's algorithm however the maximum is scaled to 1.0. In cases where one of the signals is contained entirely within another signal, the maximum value will reach 1. In the more general case the maximum will be lower and a threshold value will have to be determined to decide whether the signals are sufficiently alike. \n",
      "\n",
      "6784: Instead of looking at the phase of the individual channels, you could check the delay between both channels. Assuming that the same signal is presented to both channels, the direction of the sound source can be found from this inter-channel delay. Assuming an ear-to-ear distance of some 20cm, this delay is at most .2/340=.58ms or some 30 samples @ 48kHz. If you calculate the cross-correlation over this range (30 samples) you should find a peak indicating the source direction.\n",
      "\n",
      "To find the presence of a voice-like signal, you could calculate the total energy in the 80-1000Hz band and threshold it against some reasonable value. You can do this either in the frequency domain by summing the magnitudes in the bins from 80 to 1000Hz, or in the time-domain using a band-filter and an RMS value calculation. \n",
      "\n",
      "6784: Instead of a convolution you should use a correlation. The size of the correlation peak tells you how much both signals are alike, the position of the peak their relative position in time, or the delay between both signals.\n",
      "\n",
      "6784: If you also have the source material, you could deconvolve the recorded sound with the original and get the impulse response. From there you can calculate all kinds of acoustical parameters such as spaciousness and speech intelligibility. In general these acoustical parameters will vary from position to position.\n",
      "\n",
      "6784: You could use one of the Core Audio APIs:\n",
      "\n",
      "<pre><code>// get the device enumerator\n",
      "IMMDeviceEnumerator* pEnumerator = NULL;\n",
      "HRESULT hr = CoCreateInstance(__uuidof(MMDeviceEnumerator), NULL,\n",
      "                              CLSCTX_ALL,__uuidof(IMMDeviceEnumerator),\n",
      "                              (void**)&amp;pEnumerator);\n",
      "\n",
      "// get the endpoint collection\n",
      "IMMDeviceCollection* pCollection = NULL;\n",
      "DWORD mask = DEVICE_STATE_ACTIVE || DEVICE_STATE_UNPLUGGED;\n",
      "hr = pEnumerator-&gt;EnumAudioEndpoints(eRender, mask, &amp;pCollection);\n",
      "\n",
      "// get the size of the collection\n",
      "UINT count = 0;\n",
      "hr = pCollection-&gt;GetCount(&amp;count);\n",
      "\n",
      "for (int i = 0; i &lt; (int)count; i++)\n",
      "{\n",
      "    // get the endpoint\n",
      "    IMMDevice* pEndPoint = NULL;\n",
      "    hr = pCollection-&gt;Item(i, &amp;pEndPoint);\n",
      "\n",
      "    // get the human readable name\n",
      "    String^ friendlyName;\n",
      "    IPropertyStore* pProps = NULL;\n",
      "    HRESULT hr = pEndPoint-&gt;OpenPropertyStore(STGM_READ, &amp;pProps);\n",
      "    PROPVARIANT varName;\n",
      "    PropVariantInit(&amp;varName);\n",
      "    hr = pProps-&gt;GetValue(PKEY_Device_FriendlyName, &amp;varName);\n",
      "    friendlyName = gcnew String(varName.pwszVal);\n",
      "    PropVariantClear(&amp;varName);\n",
      "}       \n",
      "</code></pre>\n",
      "\n",
      "Error handling was removed in the above code to make it more readable. (I happen to love using C++/CLI to move between C# and the Windows APIs.)\n",
      "\n",
      "Now the harder part will be to relate the endpoint names to the MME devices in your old code base.\n",
      "\n",
      "6784: As you have 16-bit data, you should expect the signal to vary between -32768 and +32767.\n",
      "To calculate the volume you can take intervals of say 1000 samples, and calculate their RMS value. Sum the squared sample values divide by 1000 and take the square root. check this number against you threshold.\n",
      "\n",
      "6784: When you create the buffers, call <code>waveInPrepareHeader</code>. Then you can simply set the <strong>prepared</strong> flag before you call <code>waveInAddBuffer</code> on a buffer that was returned from the device.\n",
      "\n",
      "<pre><code>pHdr-&gt;dwFlags = WHDR_PREPARED;\n",
      "</code></pre>\n",
      "\n",
      "You can do this on the callback thread (or in the message handler).\n",
      "\n",
      "6784: You could low-pass filter the data at 1600 Hz (or somewhat higher, say 2k), and then resample to a lower samplerate (twice the filter frequency e.g. 4k) to reduce the number of samples. Then use zero-padding to increase the frequency resolution. \n",
      "\n",
      "6784: If your 32-bit format is float, then most likely the sample values range from -1 to 1. To convert to 16-bit (integers) you would need to multiply by 32767 and cast to INT16.\n",
      "\n",
      "If the 32-bit float range is larger than -1..1 then you need to find the minimum and maximum values and calculate a scale factor that gets the samples within the INT16 range. \n",
      "\n",
      "You may need to add some dither.\n",
      "\n",
      "6784: <ol>\n",
      "<li>Smooth the data using a lowpass filter (or average each sample with a number of surrounding samples).</li>\n",
      "<li>Find the peak in the center by looking for the highest sample value.</li>\n",
      "<li>find the valley to the right of the peak by searching for the first sample that has a higher value than its predecessor.</li>\n",
      "<li>Find the peak to the right of the valley by searching for the first sample to a smaller value than its predecessor.  </li>\n",
      "</ol>\n",
      "\n",
      "6784: MP3 is a compressed audio format. You should first decompress the data before you can use it as an audio stream comparable to the data from your microphone. The raw MP3 data has maximum entropy and should look much like white noise, which it does in you spectrogram.\n",
      "\n",
      "6784: The very low level signal where silence was expected, may have been caused by <a href=\"http://en.wikipedia.org/wiki/Dither\" rel=\"nofollow noreferrer\">dither</a> used in the conversion from 32-bit to 16-bit.\n",
      "\n",
      "6784: The reference pressure in Leq (sound pressure level) calculations is 20 micro-Pascal (rms).\n",
      "To measure absolute Leq levels, you need to calibrate your microphone using a calibrator. Most calibrators fit 1/2\" or 1/4\" microphone capsules, so I have my doubts about calibrating the microphone on an Android phone. Alternatively you may be able to use the microphone sensitivity (Pa/mV) and then calibrate the voltage level going into the ADC. Even less reliable results could be had from comparing the Android values with the measured sound level of a diffuse stationary sound field using a sound level meter.\n",
      "Note that in Leq calculations you normally use the RMS values. A single sample's value doesn't mean much. \n",
      "\n",
      "6784: You need to set the wFormat tag in the 'fmt' chunk to WAVE_FORMAT_IEEE_FLOAT (3).\n",
      "\n",
      "A good source for the WAVE format specification is <a href=\"http://www-mmsp.ece.mcgill.ca/Documents/AudioFormats/WAVE/WAVE.html\" rel=\"noreferrer\">this page</a>.\n",
      "\n",
      "6784: This scenario is described in <a href=\"http://msdn.microsoft.com/en-us/library/dd370819(VS.85).aspx\" rel=\"nofollow noreferrer\">http://msdn.microsoft.com/en-us/library/dd370819(VS.85).aspx</a>\n",
      "\n",
      "6784: The \"3-\" depends on the USB port you connect the device to. If you move the device to another USB port it will get another number. This is to uniquely identify the same devices connected to different ports. The CoreAudio API uses a true ID (much like a GUID) to identify each device, so it doesn't need to encode the differences in the device name.\n",
      "\n",
      "Mind you, this is my take on using USB sound devices and the MME and CoreAudio APIs. The true story can probably be found in the DDK, or from the folks a Microsoft. \n",
      "\n",
      "6784: On Vista and Win7 you could use the MMDevice API IMMDeviceEnumerator::GetDefaultAudioEndpoint, and then use MMDevice::GetId to get the string describing this device.\n",
      "\n",
      "On other systems (including Vista and Win7) you can use the older MME API waveInGetDevCaps and waveOutGetDevCaps using deviceID=WAVE_MAPPER (-1) to get the string describing the default device. \n",
      "\n",
      "6784: If the samplerate of your PCM data is F, then the highest frequency component in the FFT is F/2. Suppose your PCM data was sampled at 44100Hz, then your FFT values will run from 0Hz (DC) to 22050Hz. If you start with N samples, (N being a power of 2), then the FFT may return N/2 values representing all positive frequencies from 0 to F/2, or it may return N values that also include the negative frequencies from -F/2 to 0. You should check the specification of your FFT algorithm to find out to which frequency each array item is mapped. \n",
      "\n",
      "To find the peaks, you need to look at the magnitude of the FFT values. So you need to add the squared real and imaginary parts of each complex value.\n",
      "\n",
      "Suppose your FFT of N PCM samples returns N/2 complex values representing positive frequencies. Then the distance between 2 complex samples is F/2N Hz. With F=44100Hz and N=1024 samples, this would be 21.5Hz. This is your frequency resolution. If you need to find lower frequency beats, the FFT window will need to be extended.\n",
      "\n",
      "6784: You'll likely have more than 1 sample for each pixel. For each group of samples mapped to a single pixel, you could draw a (vertical) line segment from the minimum value in the sample group to the maximum value. If you zoom in to 1 sample per pixel or less, this doesn't work anymore, and the 'nice' solution would be to display the sinc interpolated values.\n",
      "Because DrawLine cannot paint a single pixel, there is a small problem when the minimum and maximum are the same. In that case you could copy a single pixel image in the desired position, as in the code below:\n",
      "\n",
      "<pre><code>double samplesPerPixel = (double)L / _width;\n",
      "double firstSample = 0;\n",
      "int endSample = firstSample + L - 1;\n",
      "for (short pixel = 0; pixel &lt; _width; pixel++)\n",
      "{\n",
      "    int lastSample = __min(endSample, (int)(firstSample + samplesPerPixel));\n",
      "    double Y = _data[channel][(int)firstSample];\n",
      "    double minY = Y;\n",
      "    double maxY = Y;\n",
      "    for (int sample = (int)firstSample + 1; sample &lt;= lastSample; sample++)\n",
      "    {\n",
      "        Y = _data[channel][sample];\n",
      "        minY = __min(Y, minY);\n",
      "        maxY = __max(Y, maxY);\n",
      "    }\n",
      "    x = pixel + _offsetx;\n",
      "    y1 = Value2Pixel(minY);\n",
      "    y2 = Value2Pixel(maxY);\n",
      "    if (y1 == y2)\n",
      "    {\n",
      "        g-&gt;DrawImageUnscaled(bm, x, y1);\n",
      "    }\n",
      "    else\n",
      "    {\n",
      "        g-&gt;DrawLine(pen, x, y1, x, y2);\n",
      "    }\n",
      "    firstSample += samplesPerPixel;\n",
      "}\n",
      "</code></pre>\n",
      "\n",
      "Note that Value2Pixel scales a sample value to a pixel value (in the y-direction).\n",
      "\n",
      "6784: Despite this MSDN page, it is possible to save a true EMF file. See this question: <a href=\"https://stackoverflow.com/questions/152729/gdi-c-how-to-save-an-image-as-emf/895204#895204\">gdi-c-how-to-save-an-image-as-emf</a>\n",
      "\n",
      "6784: The answer by erikkallen is correct. I tried this from VB.NET, and had to use 2 different DllImports to get it to work:\n",
      "\n",
      "<pre><code>&lt;System.Runtime.InteropServices.DllImportAttribute(\"gdi32.dll\", EntryPoint:=\"GetEnhMetaFileBits\")&gt; _\n",
      "    Public Shared Function GetEnhMetaFileBits(&lt;System.Runtime.InteropServices.InAttribute()&gt; ByVal hEMF As System.IntPtr, ByVal nSize As UInteger, ByVal lpData As IntPtr) As UInteger\n",
      "End Function\n",
      "\n",
      "    &lt;System.Runtime.InteropServices.DllImportAttribute(\"gdi32.dll\", EntryPoint:=\"GetEnhMetaFileBits\")&gt; _\n",
      "    Public Shared Function GetEnhMetaFileBits(&lt;System.Runtime.InteropServices.InAttribute()&gt; ByVal hEMF As System.IntPtr, ByVal nSize As UInteger, ByVal lpData() As Byte) As UInteger\n",
      "End Function\n",
      "</code></pre>\n",
      "\n",
      "The first import is used for the first call to get the emf size. The second import to get the actual bits.\n",
      "Alternatively you could use:\n",
      "\n",
      "<pre><code>Dim h As IntPtr = mf.GetHenhmetafile()\n",
      "CopyEnhMetaFileW(h, FileName)\n",
      "</code></pre>\n",
      "\n",
      "This copies the emf bits directly to the named file.\n",
      "\n",
      "6784: I ended up using the WebBrowser component to load the .html file, and thereby trigger the GA tracker. The WebBrowser component executes the embedded JavaScript. \n",
      "\n",
      "<pre><code>using (WebBrowser wb = new WebBrowser())\n",
      "{\n",
      "    wb.Url = new Uri(@\"mytrackingpage.html\");\n",
      "    while (wb.ReadyState != WebBrowserReadyState.Complete)\n",
      "    {\n",
      "        Application.DoEvents();\n",
      "    }\n",
      "} \n",
      "</code></pre>\n",
      "\n",
      "Now all I have to do is to add some errorhandling, get rid of the ugly DoEvents and move the WebBrowser to a separate thread.\n",
      "\n",
      "6784: You could use:\n",
      "\n",
      "<pre><code>int mixerId = -1;\n",
      "int inputID = MmeMixerApi.WAVE_MAPPER; // = -1\n",
      "int result = MmeMixerApi.mixerGetID(inputId, ref mixerId, MIXER_OBJECTFLAG.WAVEIN);\n",
      "</code></pre>\n",
      "\n",
      "The default input and output devices can be accessed through the wave mapper which has an ID of -1. mixerGetID will return the mixer ID associated with that input. You can then use the mixer ID to iterate over the controls. You would still need to find the correct source line (e.g. microphone, line-in etc.). For this you may want to look for a source line with a particular dwComponentType such as MIXERLINE_COMPONENTTYPE.SRC_MICROPHONE or MIXERLINE_COMPONENTTYPE.SRC_LINE. \n",
      "\n",
      "6784: As MSN said the samples are in 32-bit floats. To detect a silence you would normally calculate the RMS value: Take the average of the <strong>squared</strong> sample values over some time interval (say 20-50 ms) and compare (square root of) this average to a threshold.\n",
      "The noise inherent in the microphone signal may let single samples reach above the threshold while the ambient sound would still be considered silence. The averaging over a short interval will result in a value that corresponds better with our perception.\n",
      "\n",
      "6784: You can use something like the following in the post-build event:\n",
      "\n",
      "Powershell -File \"$(SolutionDir)PostBuild.ps1\" $(PlatformName)\n",
      "\n",
      "The platform name can then be found in $args[0] inside the script (PostBuild.ps1).\n",
      "\n",
      "6784: Assuming you have the ID of the input and output devices, you could use something like the following to get the corresponding mixer IDs. If both are the same, both are attached to the same mixer, and most likely part of the same physical hardware.\n",
      "\n",
      "<pre><code>    /// &lt;summary&gt;\n",
      "    /// Get the ID of the mixer associated with the given input device ID\n",
      "    /// Returns -1 if no such mixer can be found\n",
      "    /// &lt;/summary&gt;\n",
      "    static public int GetMixerIdInput(int inputId)\n",
      "    {\n",
      "        int mixerId = -1;\n",
      "        int result = MmeMixerApi.mixerGetID(inputId, ref mixerId, MIXER_OBJECTFLAG.WAVEIN);\n",
      "        if (((MMError)result != MMError.MMSYSERR_NOERROR) &amp;&amp;\n",
      "            ((MMError)result != MMError.MMSYSERR_NODRIVER))\n",
      "        {\n",
      "            throw new MmeException((MMError)result);\n",
      "        }\n",
      "        return mixerId;\n",
      "    }\n",
      "\n",
      "    /// &lt;summary&gt;\n",
      "    /// Get the ID of the mixer associated with the given output device ID\n",
      "    /// Returns -1 if no such mixer can be found\n",
      "    /// &lt;/summary&gt;\n",
      "    static public int GetMixerIdOutput(int outputId)\n",
      "    {\n",
      "        int mixerId = -1;\n",
      "        int result = MmeMixerApi.mixerGetID(outputId, ref mixerId, MIXER_OBJECTFLAG.WAVEOUT);\n",
      "        if (((MMError)result != MMError.MMSYSERR_NOERROR) &amp;&amp;\n",
      "            ((MMError)result != MMError.MMSYSERR_NODRIVER))\n",
      "        {\n",
      "            throw new MmeException((MMError)result);\n",
      "        }\n",
      "        return mixerId;\n",
      "    }\n",
      "</code></pre>\n",
      "\n",
      "If you only have the name for the input device, you can use something like the following to find the device ID:\n",
      "\n",
      "<pre><code>    /// &lt;summary&gt;\n",
      "    /// Find the ID of the input device given a name\n",
      "    /// &lt;/summary&gt;\n",
      "    static public int GetWaveInputId(string name)\n",
      "    {\n",
      "        int id = MmeWaveApi.WAVE_MAPPER;\n",
      "        int devCount = MmeWaveApi.waveInGetNumDevs();\n",
      "        WAVEINCAPS caps = new WAVEINCAPS();\n",
      "        for (int dev = 0; (dev &lt; devCount) &amp;&amp; (id == MmeWaveApi.WAVE_MAPPER); dev++)\n",
      "        {\n",
      "            int result = MmeWaveApi.waveInGetDevCaps(dev, ref caps, Marshal.SizeOf(caps));\n",
      "            if ((MMError)result == MMError.MMSYSERR_NOERROR)\n",
      "            {\n",
      "                if (string.Compare(name, 0, caps.szPname, 0, Math.Min(name.Length, caps.szPname.Length)) == 0)\n",
      "                {\n",
      "                    id = dev;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        return id;\n",
      "    }\n",
      "</code></pre>\n",
      "\n",
      "6784: The straightforward way would be to interpret the wav file headers, extract the samples and interleave the samples from both files into a new sample stream which you write to a new wav file. You could also add the sample values of both source wav files sample by sample to 'mix' both files into a single track.\n",
      "The wav file format is not that complicated so it should not be too difficult to write this code. Also, there are lots of open source projects containing code to read and write wav files.\n",
      "Note that things can get somewhat trickier of both source wav files have different sample rates.\n",
      "Check <a href=\"http://www-mmsp.ece.mcgill.ca/Documents/AudioFormats/WAVE/WAVE.html\" rel=\"nofollow noreferrer\">this</a> for a description of the wav format.\n",
      "\n",
      "6473: The sound pressure decreases by 1/r. So a doubling of the distance results in a 6 dB lower amplitude. This should be easy to model by a distance dependent amplification.\n",
      "\n",
      "The interesting part of the problem is the sound absorption caused by air. This absorption is frequency dependent (it is higher for high frequencies) and also depends or air pressure, humidity and temperature. You can find a detailed quantitative model in the ISO 9613-1 standard.    \n",
      "\n",
      "6473: Your findMaxAmplitude only looks at the positive excursions. It should use something like\n",
      "\n",
      "<pre><code>max = (short)Math.Max(max, Math.Abs(value));\n",
      "</code></pre>\n",
      "\n",
      "Your normalization seems quite involved. A simpler version would use:\n",
      "\n",
      "<pre><code>return (short)Math.Round(value * targetMax / rawMax);\n",
      "</code></pre>\n",
      "\n",
      "Whether a targetMax of 8000 is correct is a matter of taste. Normally I would expect normalisation of 16-bit samples to use the maximum range of values. So a targetMax of 32767 seems more logical.\n",
      "The normalization should probably be done after the LPF operation, as the gain of the LPF may change the maximum value of your sequence.\n",
      "\n",
      "6473: The ASIO API defines the following call on the ASIO driver:\n",
      "\n",
      "<pre><code>ASIOError ASIOControlPanel(void);\n",
      "</code></pre>\n",
      "\n",
      "Use this to show the control panel.\n",
      "\n",
      "6473: When measuring the level of a sound signal, you should calculate the dB from the RMS value. In your sample you are looking at the absolute peak level. A single (peak) sample value determines your dB value, even when all other samples are exactly 0.\n",
      "\n",
      "try this:\n",
      "\n",
      "<pre><code>double sum = 0;\n",
      "for (var i = 0; i &lt; _buffer.length; i = i + 2)\n",
      "{\n",
      "    double sample = BitConverter.ToInt16(_buffer, i) / 32768.0;\n",
      "    sum += (sample * sample);\n",
      "}\n",
      "double rms = Math.Sqrt(sum / (_buffer.length / 2));\n",
      "var decibel = 20 * Math.Log10(rms);\n",
      "</code></pre>\n",
      "\n",
      "For 'instantaneous' dB levels you would normally calculate the RMS over a segment of 20-50 ms.\n",
      "Note that the calculated dB value is relative to full-scale. For sound the dB value should be related to 20 uPa, and you will need to calibrate your signal to find the proper conversion from digital values to pressure values.\n",
      "\n",
      "6473: The waveOut API only accepts interleaved samples for the different channels, so you will have to merge the 2 single channel files into a dual channel file and play that.\n",
      "\n",
      "6473: The problem and solution are explained in more detail here: <a href=\"http://social.msdn.microsoft.com/Forums/en-US/winforms/thread/77fbec3b-1f63-42e1-a200-19b261b63794/\" rel=\"nofollow\">Dev center forums</a>.\n",
      "This also explains why setting Me.Focus helps fixing the problem.\n",
      "\n",
      "6473: If you use the maximum amplitude (2), then your volume level would be determined by a single sample (which you might not even be able to hear). When calculating a value that correlates with your impression of the loudness of the sound such as the Sound Pressure Level or the Sound Power Level you need to use the RMS (1).\n",
      "\n",
      "Because you ear is not equally sensitive to all frequencies, a better correlate of your perception can be had by using an <a href=\"http://en.wikipedia.org/wiki/A-weighting\" rel=\"nofollow\">A-weighting</a> on the signal. Split (filter) the signal in octave bands, calculate the RMS for each band and apply the A-weighting. \n",
      "\n",
      "6473: I don't know if this is still print: \"Voice and speech processing' by Thomas Parsons. ISBN 0-07-048541-0. If you search for this book in Google books you'll quickly find related/equivalent works. \n",
      "\n",
      "6473: The CF_WAVE clipboard format is simply a .wav file. If you load a .wav file into memory, then you can copy it to the clipboard using code like this:\n",
      "\n",
      "<pre><code>                DWORD BytesRead;\n",
      "            void* pFData = GlobalLock(hFData);\n",
      "            if (ReadFile(hFile,pFData,GetFileSize(hFile,NULL),&amp;BytesRead,NULL) == FALSE)\n",
      "            {\n",
      "                ErrCode = E_FILE_READ;\n",
      "            }\n",
      "            GlobalUnlock(hFData);\n",
      "</code></pre>\n",
      "\n",
      "...\n",
      "\n",
      "<pre><code>        if (OpenClipboard(NULL))\n",
      "    {\n",
      "        EmptyClipboard();\n",
      "        if (SetClipboardData(CF_WAVE,hFData) == NULL)\n",
      "        {\n",
      "            GlobalFree(hFData);\n",
      "        }\n",
      "        CloseClipboard();\n",
      "    }\n",
      "    else\n",
      "    {\n",
      "</code></pre>\n",
      "\n",
      "Here, hFile the the WAV file handle.\n",
      "\n",
      "6473: You could use (full- or third-) octave band filters and use tabulated values to perform the A- or C- weighting for each band. Calculate the signal level in each band and add the tabulated weighting values found <a href=\"http://www.sengpielaudio.com/calculator-dba-spl.htm\" rel=\"nofollow\">here</a>.\n",
      "\n",
      "6473: mmioDescend begins searching at the current file position. So it will not find the chunk you have just written. You should first reset the file position to the start, or to a previous chunk.\n",
      "Did you set the other chunk fields (ckSize and fccType) correctly? \n",
      "Perhaps you should perform an mmioAscend after creating the chunk to get the padding right.\n",
      "\n",
      "6473: One way to do this would be to resample the signal from 44100 Hz to 48000 Hz, so both signals have the same samplerate, and perform a cross-correlation. The shape of the cross-correlation could be a measure of similarity. You could look at the height of the peak, or the ratio of energy in the peak to the total energy.\n",
      "\n",
      "Note however that when the signal repeats itself, you will get multiple cross-correlation peaks.\n",
      "\n",
      "6473: In sound, decibel values are referenced to a sound pressure level of 20µPa (20 micro Pascal).\n",
      "So in your case the reference_amplitude would be the amplitude generated by your microphone in the presence of a sound field with a level of 20µPa. \n",
      "\n",
      "In practice, to find this level, microphones are often calibrated (using a microphone calibrator) with a signal of some precisely known level (often around 94dB). The amplitude resulting from this calibration signal can then be used to calculate the amplitude for the reference signal (assuming the response of the microphone is linear).   \n",
      "\n",
      "6473: To change the pitch, you'll have to perform an <a href=\"http://en.wikipedia.org/wiki/FFT\" rel=\"nofollow\">FFT</a> on a number of frames and then shift the data in frequency (move the data to different frequency bins) and perform an inverse FFT.\n",
      "\n",
      "If you don't mind the sound fragment getting longer while lowering the pitch (or higher when increasing the pitch), you could <a href=\"http://en.wikipedia.org/wiki/Sample_rate_conversion\" rel=\"nofollow\">resample</a> the frames. For instance, you could double each frame (insert a copy of each frame in the stream) thereby lowering the playback speed and the pitch. You can then improve the audio quality by improving the resampling algorithm to use some sort of interpolation and/or filtering. \n",
      "\n",
      "6473: I do not know whether a formal definition of a frame exists, but when referring to an audio  frame we usually mean a single time sample of a number of channels. So 2 audio channels @ 8 bits per channel results in 2 bytes per frame. 4 channels @ 16 bit per sample is 8 bytes. \n",
      "\n",
      "6473: There's a lot of filter source code to be found <a href=\"http://www.musicdsp.org/archive.php?classid=3\" rel=\"nofollow\">here</a>\n",
      "\n",
      "6473: Here we go for the bounty :)\n",
      "\n",
      "To find a particular reference signal in a larger audio fragment, you need to use a cross-correlation algorithm. The basic formulae can be found in this <a href=\"http://en.wikipedia.org/wiki/Cross_correlation\" rel=\"nofollow\">Wikipedia article</a>.\n",
      "\n",
      "Cross-correlation is a process by which 2 signals are compared. This is done by multiplying both signals and summing the results for all samples. Then one of the signals is shifted (usually by 1 sample), and the calculation is repeated. If you try to visualize this for very simple signals such as a single impulse (e.g. 1 sample has a certain value while the remaining samples are zero), or a pure sine wave, you will see that the result of the cross-correlation is indeed a measure for for how much both signals are alike and the delay between them. Another article that may provide more insight can be found <a href=\"http://paulbourke.net/miscellaneous/correlate/\" rel=\"nofollow\">here</a>. \n",
      "\n",
      "This <a href=\"http://paulbourke.net/miscellaneous/correlate/\" rel=\"nofollow\">article by Paul Bourke</a> also contains source code for a straightforward time-domain implementation. Note that the article is written for a general signal. Audio has the special property that the long-time average is usualy 0. This means that the averages used in Paul Bourkes formula (mx and my) can be left out.\n",
      "There are also fast implementations of the cross-correlation based on the FFT (see <a href=\"http://www.alglib.net/\" rel=\"nofollow\">ALGLIB</a>).\n",
      "\n",
      "The (maximum) value of the correlation depends on the sample values in the audio signals. In Paul Bourke's algorithm however the maximum is scaled to 1.0. In cases where one of the signals is contained entirely within another signal, the maximum value will reach 1. In the more general case the maximum will be lower and a threshold value will have to be determined to decide whether the signals are sufficiently alike. \n",
      "\n",
      "6473: Instead of looking at the phase of the individual channels, you could check the delay between both channels. Assuming that the same signal is presented to both channels, the direction of the sound source can be found from this inter-channel delay. Assuming an ear-to-ear distance of some 20cm, this delay is at most .2/340=.58ms or some 30 samples @ 48kHz. If you calculate the cross-correlation over this range (30 samples) you should find a peak indicating the source direction.\n",
      "\n",
      "To find the presence of a voice-like signal, you could calculate the total energy in the 80-1000Hz band and threshold it against some reasonable value. You can do this either in the frequency domain by summing the magnitudes in the bins from 80 to 1000Hz, or in the time-domain using a band-filter and an RMS value calculation. \n",
      "\n",
      "6473: Instead of a convolution you should use a correlation. The size of the correlation peak tells you how much both signals are alike, the position of the peak their relative position in time, or the delay between both signals.\n",
      "\n",
      "6473: If you also have the source material, you could deconvolve the recorded sound with the original and get the impulse response. From there you can calculate all kinds of acoustical parameters such as spaciousness and speech intelligibility. In general these acoustical parameters will vary from position to position.\n",
      "\n",
      "6473: You could use one of the Core Audio APIs:\n",
      "\n",
      "<pre><code>// get the device enumerator\n",
      "IMMDeviceEnumerator* pEnumerator = NULL;\n",
      "HRESULT hr = CoCreateInstance(__uuidof(MMDeviceEnumerator), NULL,\n",
      "                              CLSCTX_ALL,__uuidof(IMMDeviceEnumerator),\n",
      "                              (void**)&amp;pEnumerator);\n",
      "\n",
      "// get the endpoint collection\n",
      "IMMDeviceCollection* pCollection = NULL;\n",
      "DWORD mask = DEVICE_STATE_ACTIVE || DEVICE_STATE_UNPLUGGED;\n",
      "hr = pEnumerator-&gt;EnumAudioEndpoints(eRender, mask, &amp;pCollection);\n",
      "\n",
      "// get the size of the collection\n",
      "UINT count = 0;\n",
      "hr = pCollection-&gt;GetCount(&amp;count);\n",
      "\n",
      "for (int i = 0; i &lt; (int)count; i++)\n",
      "{\n",
      "    // get the endpoint\n",
      "    IMMDevice* pEndPoint = NULL;\n",
      "    hr = pCollection-&gt;Item(i, &amp;pEndPoint);\n",
      "\n",
      "    // get the human readable name\n",
      "    String^ friendlyName;\n",
      "    IPropertyStore* pProps = NULL;\n",
      "    HRESULT hr = pEndPoint-&gt;OpenPropertyStore(STGM_READ, &amp;pProps);\n",
      "    PROPVARIANT varName;\n",
      "    PropVariantInit(&amp;varName);\n",
      "    hr = pProps-&gt;GetValue(PKEY_Device_FriendlyName, &amp;varName);\n",
      "    friendlyName = gcnew String(varName.pwszVal);\n",
      "    PropVariantClear(&amp;varName);\n",
      "}       \n",
      "</code></pre>\n",
      "\n",
      "Error handling was removed in the above code to make it more readable. (I happen to love using C++/CLI to move between C# and the Windows APIs.)\n",
      "\n",
      "Now the harder part will be to relate the endpoint names to the MME devices in your old code base.\n",
      "\n",
      "6473: As you have 16-bit data, you should expect the signal to vary between -32768 and +32767.\n",
      "To calculate the volume you can take intervals of say 1000 samples, and calculate their RMS value. Sum the squared sample values divide by 1000 and take the square root. check this number against you threshold.\n",
      "\n",
      "6473: When you create the buffers, call <code>waveInPrepareHeader</code>. Then you can simply set the <strong>prepared</strong> flag before you call <code>waveInAddBuffer</code> on a buffer that was returned from the device.\n",
      "\n",
      "<pre><code>pHdr-&gt;dwFlags = WHDR_PREPARED;\n",
      "</code></pre>\n",
      "\n",
      "You can do this on the callback thread (or in the message handler).\n",
      "\n",
      "6473: You could low-pass filter the data at 1600 Hz (or somewhat higher, say 2k), and then resample to a lower samplerate (twice the filter frequency e.g. 4k) to reduce the number of samples. Then use zero-padding to increase the frequency resolution. \n",
      "\n",
      "6473: If your 32-bit format is float, then most likely the sample values range from -1 to 1. To convert to 16-bit (integers) you would need to multiply by 32767 and cast to INT16.\n",
      "\n",
      "If the 32-bit float range is larger than -1..1 then you need to find the minimum and maximum values and calculate a scale factor that gets the samples within the INT16 range. \n",
      "\n",
      "You may need to add some dither.\n",
      "\n",
      "6473: <ol>\n",
      "<li>Smooth the data using a lowpass filter (or average each sample with a number of surrounding samples).</li>\n",
      "<li>Find the peak in the center by looking for the highest sample value.</li>\n",
      "<li>find the valley to the right of the peak by searching for the first sample that has a higher value than its predecessor.</li>\n",
      "<li>Find the peak to the right of the valley by searching for the first sample to a smaller value than its predecessor.  </li>\n",
      "</ol>\n",
      "\n",
      "6473: MP3 is a compressed audio format. You should first decompress the data before you can use it as an audio stream comparable to the data from your microphone. The raw MP3 data has maximum entropy and should look much like white noise, which it does in you spectrogram.\n",
      "\n",
      "6473: The very low level signal where silence was expected, may have been caused by <a href=\"http://en.wikipedia.org/wiki/Dither\" rel=\"nofollow noreferrer\">dither</a> used in the conversion from 32-bit to 16-bit.\n",
      "\n",
      "6473: The reference pressure in Leq (sound pressure level) calculations is 20 micro-Pascal (rms).\n",
      "To measure absolute Leq levels, you need to calibrate your microphone using a calibrator. Most calibrators fit 1/2\" or 1/4\" microphone capsules, so I have my doubts about calibrating the microphone on an Android phone. Alternatively you may be able to use the microphone sensitivity (Pa/mV) and then calibrate the voltage level going into the ADC. Even less reliable results could be had from comparing the Android values with the measured sound level of a diffuse stationary sound field using a sound level meter.\n",
      "Note that in Leq calculations you normally use the RMS values. A single sample's value doesn't mean much. \n",
      "\n",
      "6473: You need to set the wFormat tag in the 'fmt' chunk to WAVE_FORMAT_IEEE_FLOAT (3).\n",
      "\n",
      "A good source for the WAVE format specification is <a href=\"http://www-mmsp.ece.mcgill.ca/Documents/AudioFormats/WAVE/WAVE.html\" rel=\"noreferrer\">this page</a>.\n",
      "\n",
      "6473: This scenario is described in <a href=\"http://msdn.microsoft.com/en-us/library/dd370819(VS.85).aspx\" rel=\"nofollow noreferrer\">http://msdn.microsoft.com/en-us/library/dd370819(VS.85).aspx</a>\n",
      "\n",
      "6473: The \"3-\" depends on the USB port you connect the device to. If you move the device to another USB port it will get another number. This is to uniquely identify the same devices connected to different ports. The CoreAudio API uses a true ID (much like a GUID) to identify each device, so it doesn't need to encode the differences in the device name.\n",
      "\n",
      "Mind you, this is my take on using USB sound devices and the MME and CoreAudio APIs. The true story can probably be found in the DDK, or from the folks a Microsoft. \n",
      "\n",
      "6473: On Vista and Win7 you could use the MMDevice API IMMDeviceEnumerator::GetDefaultAudioEndpoint, and then use MMDevice::GetId to get the string describing this device.\n",
      "\n",
      "On other systems (including Vista and Win7) you can use the older MME API waveInGetDevCaps and waveOutGetDevCaps using deviceID=WAVE_MAPPER (-1) to get the string describing the default device. \n",
      "\n",
      "6473: If the samplerate of your PCM data is F, then the highest frequency component in the FFT is F/2. Suppose your PCM data was sampled at 44100Hz, then your FFT values will run from 0Hz (DC) to 22050Hz. If you start with N samples, (N being a power of 2), then the FFT may return N/2 values representing all positive frequencies from 0 to F/2, or it may return N values that also include the negative frequencies from -F/2 to 0. You should check the specification of your FFT algorithm to find out to which frequency each array item is mapped. \n",
      "\n",
      "To find the peaks, you need to look at the magnitude of the FFT values. So you need to add the squared real and imaginary parts of each complex value.\n",
      "\n",
      "Suppose your FFT of N PCM samples returns N/2 complex values representing positive frequencies. Then the distance between 2 complex samples is F/2N Hz. With F=44100Hz and N=1024 samples, this would be 21.5Hz. This is your frequency resolution. If you need to find lower frequency beats, the FFT window will need to be extended.\n",
      "\n",
      "6473: You'll likely have more than 1 sample for each pixel. For each group of samples mapped to a single pixel, you could draw a (vertical) line segment from the minimum value in the sample group to the maximum value. If you zoom in to 1 sample per pixel or less, this doesn't work anymore, and the 'nice' solution would be to display the sinc interpolated values.\n",
      "Because DrawLine cannot paint a single pixel, there is a small problem when the minimum and maximum are the same. In that case you could copy a single pixel image in the desired position, as in the code below:\n",
      "\n",
      "<pre><code>double samplesPerPixel = (double)L / _width;\n",
      "double firstSample = 0;\n",
      "int endSample = firstSample + L - 1;\n",
      "for (short pixel = 0; pixel &lt; _width; pixel++)\n",
      "{\n",
      "    int lastSample = __min(endSample, (int)(firstSample + samplesPerPixel));\n",
      "    double Y = _data[channel][(int)firstSample];\n",
      "    double minY = Y;\n",
      "    double maxY = Y;\n",
      "    for (int sample = (int)firstSample + 1; sample &lt;= lastSample; sample++)\n",
      "    {\n",
      "        Y = _data[channel][sample];\n",
      "        minY = __min(Y, minY);\n",
      "        maxY = __max(Y, maxY);\n",
      "    }\n",
      "    x = pixel + _offsetx;\n",
      "    y1 = Value2Pixel(minY);\n",
      "    y2 = Value2Pixel(maxY);\n",
      "    if (y1 == y2)\n",
      "    {\n",
      "        g-&gt;DrawImageUnscaled(bm, x, y1);\n",
      "    }\n",
      "    else\n",
      "    {\n",
      "        g-&gt;DrawLine(pen, x, y1, x, y2);\n",
      "    }\n",
      "    firstSample += samplesPerPixel;\n",
      "}\n",
      "</code></pre>\n",
      "\n",
      "Note that Value2Pixel scales a sample value to a pixel value (in the y-direction).\n",
      "\n",
      "6473: Despite this MSDN page, it is possible to save a true EMF file. See this question: <a href=\"https://stackoverflow.com/questions/152729/gdi-c-how-to-save-an-image-as-emf/895204#895204\">gdi-c-how-to-save-an-image-as-emf</a>\n",
      "\n",
      "6473: The answer by erikkallen is correct. I tried this from VB.NET, and had to use 2 different DllImports to get it to work:\n",
      "\n",
      "<pre><code>&lt;System.Runtime.InteropServices.DllImportAttribute(\"gdi32.dll\", EntryPoint:=\"GetEnhMetaFileBits\")&gt; _\n",
      "    Public Shared Function GetEnhMetaFileBits(&lt;System.Runtime.InteropServices.InAttribute()&gt; ByVal hEMF As System.IntPtr, ByVal nSize As UInteger, ByVal lpData As IntPtr) As UInteger\n",
      "End Function\n",
      "\n",
      "    &lt;System.Runtime.InteropServices.DllImportAttribute(\"gdi32.dll\", EntryPoint:=\"GetEnhMetaFileBits\")&gt; _\n",
      "    Public Shared Function GetEnhMetaFileBits(&lt;System.Runtime.InteropServices.InAttribute()&gt; ByVal hEMF As System.IntPtr, ByVal nSize As UInteger, ByVal lpData() As Byte) As UInteger\n",
      "End Function\n",
      "</code></pre>\n",
      "\n",
      "The first import is used for the first call to get the emf size. The second import to get the actual bits.\n",
      "Alternatively you could use:\n",
      "\n",
      "<pre><code>Dim h As IntPtr = mf.GetHenhmetafile()\n",
      "CopyEnhMetaFileW(h, FileName)\n",
      "</code></pre>\n",
      "\n",
      "This copies the emf bits directly to the named file.\n",
      "\n",
      "6473: I ended up using the WebBrowser component to load the .html file, and thereby trigger the GA tracker. The WebBrowser component executes the embedded JavaScript. \n",
      "\n",
      "<pre><code>using (WebBrowser wb = new WebBrowser())\n",
      "{\n",
      "    wb.Url = new Uri(@\"mytrackingpage.html\");\n",
      "    while (wb.ReadyState != WebBrowserReadyState.Complete)\n",
      "    {\n",
      "        Application.DoEvents();\n",
      "    }\n",
      "} \n",
      "</code></pre>\n",
      "\n",
      "Now all I have to do is to add some errorhandling, get rid of the ugly DoEvents and move the WebBrowser to a separate thread.\n",
      "\n",
      "6473: You could use:\n",
      "\n",
      "<pre><code>int mixerId = -1;\n",
      "int inputID = MmeMixerApi.WAVE_MAPPER; // = -1\n",
      "int result = MmeMixerApi.mixerGetID(inputId, ref mixerId, MIXER_OBJECTFLAG.WAVEIN);\n",
      "</code></pre>\n",
      "\n",
      "The default input and output devices can be accessed through the wave mapper which has an ID of -1. mixerGetID will return the mixer ID associated with that input. You can then use the mixer ID to iterate over the controls. You would still need to find the correct source line (e.g. microphone, line-in etc.). For this you may want to look for a source line with a particular dwComponentType such as MIXERLINE_COMPONENTTYPE.SRC_MICROPHONE or MIXERLINE_COMPONENTTYPE.SRC_LINE. \n",
      "\n",
      "6473: As MSN said the samples are in 32-bit floats. To detect a silence you would normally calculate the RMS value: Take the average of the <strong>squared</strong> sample values over some time interval (say 20-50 ms) and compare (square root of) this average to a threshold.\n",
      "The noise inherent in the microphone signal may let single samples reach above the threshold while the ambient sound would still be considered silence. The averaging over a short interval will result in a value that corresponds better with our perception.\n",
      "\n",
      "6473: You can use something like the following in the post-build event:\n",
      "\n",
      "Powershell -File \"$(SolutionDir)PostBuild.ps1\" $(PlatformName)\n",
      "\n",
      "The platform name can then be found in $args[0] inside the script (PostBuild.ps1).\n",
      "\n",
      "6473: Assuming you have the ID of the input and output devices, you could use something like the following to get the corresponding mixer IDs. If both are the same, both are attached to the same mixer, and most likely part of the same physical hardware.\n",
      "\n",
      "<pre><code>    /// &lt;summary&gt;\n",
      "    /// Get the ID of the mixer associated with the given input device ID\n",
      "    /// Returns -1 if no such mixer can be found\n",
      "    /// &lt;/summary&gt;\n",
      "    static public int GetMixerIdInput(int inputId)\n",
      "    {\n",
      "        int mixerId = -1;\n",
      "        int result = MmeMixerApi.mixerGetID(inputId, ref mixerId, MIXER_OBJECTFLAG.WAVEIN);\n",
      "        if (((MMError)result != MMError.MMSYSERR_NOERROR) &amp;&amp;\n",
      "            ((MMError)result != MMError.MMSYSERR_NODRIVER))\n",
      "        {\n",
      "            throw new MmeException((MMError)result);\n",
      "        }\n",
      "        return mixerId;\n",
      "    }\n",
      "\n",
      "    /// &lt;summary&gt;\n",
      "    /// Get the ID of the mixer associated with the given output device ID\n",
      "    /// Returns -1 if no such mixer can be found\n",
      "    /// &lt;/summary&gt;\n",
      "    static public int GetMixerIdOutput(int outputId)\n",
      "    {\n",
      "        int mixerId = -1;\n",
      "        int result = MmeMixerApi.mixerGetID(outputId, ref mixerId, MIXER_OBJECTFLAG.WAVEOUT);\n",
      "        if (((MMError)result != MMError.MMSYSERR_NOERROR) &amp;&amp;\n",
      "            ((MMError)result != MMError.MMSYSERR_NODRIVER))\n",
      "        {\n",
      "            throw new MmeException((MMError)result);\n",
      "        }\n",
      "        return mixerId;\n",
      "    }\n",
      "</code></pre>\n",
      "\n",
      "If you only have the name for the input device, you can use something like the following to find the device ID:\n",
      "\n",
      "<pre><code>    /// &lt;summary&gt;\n",
      "    /// Find the ID of the input device given a name\n",
      "    /// &lt;/summary&gt;\n",
      "    static public int GetWaveInputId(string name)\n",
      "    {\n",
      "        int id = MmeWaveApi.WAVE_MAPPER;\n",
      "        int devCount = MmeWaveApi.waveInGetNumDevs();\n",
      "        WAVEINCAPS caps = new WAVEINCAPS();\n",
      "        for (int dev = 0; (dev &lt; devCount) &amp;&amp; (id == MmeWaveApi.WAVE_MAPPER); dev++)\n",
      "        {\n",
      "            int result = MmeWaveApi.waveInGetDevCaps(dev, ref caps, Marshal.SizeOf(caps));\n",
      "            if ((MMError)result == MMError.MMSYSERR_NOERROR)\n",
      "            {\n",
      "                if (string.Compare(name, 0, caps.szPname, 0, Math.Min(name.Length, caps.szPname.Length)) == 0)\n",
      "                {\n",
      "                    id = dev;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        return id;\n",
      "    }\n",
      "</code></pre>\n",
      "\n",
      "6473: The straightforward way would be to interpret the wav file headers, extract the samples and interleave the samples from both files into a new sample stream which you write to a new wav file. You could also add the sample values of both source wav files sample by sample to 'mix' both files into a single track.\n",
      "The wav file format is not that complicated so it should not be too difficult to write this code. Also, there are lots of open source projects containing code to read and write wav files.\n",
      "Note that things can get somewhat trickier of both source wav files have different sample rates.\n",
      "Check <a href=\"http://www-mmsp.ece.mcgill.ca/Documents/AudioFormats/WAVE/WAVE.html\" rel=\"nofollow noreferrer\">this</a> for a description of the wav format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "user_list=[6784, 6473]\n",
    "\n",
    "answers=SITE.fetch('users/{ids}/answers', ids = user_list, filter='withbody')\n",
    "\n",
    "for i in user_list:\n",
    "    for a in answers['items']:\n",
    "        full=a['body']\n",
    "        print(f'{i}: {(full.replace(\"<p>\",\"\")).replace(\"</p>\",\"\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python389jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
